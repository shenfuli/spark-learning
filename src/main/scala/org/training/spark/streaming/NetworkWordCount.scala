package org.training.spark.streaming

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * 0： 下载spark code
  * git clone -b v1.6.3 git@github.com:apache/spark.git
  * 1： 登陆linux 系统执行下面的命令
  * [root@slave3 ~]# nc -lk 9999
  * hello world hello me hello you
  * 2: 启动Spark Streaming 程序
  * 3: 查看结果
  * (hello,3)
  * (me,1)
  * (world,1)
  * (you,1)
  *
  * 参考资料：
  * [1]  streaming-programming-guide. http://spark.apache.org/docs/1.6.3/streaming-programming-guide.html
  * Created by fuli.shen on 2017/5/19.
  */
object NetworkWordCount {
  def main(args: Array[String]) {
    // 10.4.1.22  9999
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    // Create a local StreamingContext with two working thread and batch interval of 10 second.
    // The master requires 2 cores to prevent from a starvation scenario.
    //  StreamingContext is the main entry point for all streaming functionality
    val sparkConf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(10))

    val hostname = args(0)
    val port = args(1)
    // Create a socket stream on target ip:port => like localhost:9999
    // and count the words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(hostname, port.toInt, StorageLevel.MEMORY_AND_DISK_SER)
    // Split each line into words
    // flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream
    val words = lines.flatMap(_.split(" "))
    // Count each word in each batch
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    // print a few of the counts generated every 10second
    wordCounts.print()
    // Spark Streaming only sets up the computation it will perform when it is started.and no real processing has started yet
    ssc.start()
    //To start the processing after all the transformations have been setup, we finally call
    ssc.awaitTermination()
  }
}
